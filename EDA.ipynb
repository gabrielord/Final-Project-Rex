{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabriellima/Documents/CentraleSupelec/Mention/Rexia/Final Project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/gabriellima/Documents/CentraleSupelec/Mention/Rexia/Final Project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from xplique.attributions import (\n",
    "    Saliency, IntegratedGradients,\n",
    "    Occlusion, KernelShap, Lime\n",
    ")\n",
    "from xplique.metrics import Insertion, Deletion\n",
    "\n",
    "import data_manipulation\n",
    "import model\n",
    "import explicability_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "base = \"https://zenodo.org/record/2546921/files\"\n",
    "files = [\n",
    "    \"camelyonpatch_level_2_split_train_x.h5\",\n",
    "    \"camelyonpatch_level_2_split_train_y.h5\",\n",
    "    \"camelyonpatch_level_2_split_valid_x.h5\",\n",
    "    \"camelyonpatch_level_2_split_valid_y.h5\",\n",
    "    \"camelyonpatch_level_2_split_test_x.h5\",\n",
    "    \"camelyonpatch_level_2_split_test_y.h5\",\n",
    "]\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "for fname in files:\n",
    "    out_path = os.path.join(\"data\", fname)\n",
    "    if not os.path.exists(out_path):\n",
    "        print(f\"Downloading {fname}...\")\n",
    "        url = f\"{base}/{fname}.gz?download=1\"\n",
    "        os.system(f\"curl -L -o {out_path} '{url}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.2, 0.2, 0.2)], p=0.5)\n",
    "])\n",
    "\n",
    "# Downloaded files \n",
    "train_x = \"data/camelyonpatch_level_2_split_train_x.h5\"\n",
    "train_y = \"data/camelyonpatch_level_2_split_train_y.h5\"\n",
    "valid_x = \"data/camelyonpatch_level_2_split_valid_x.h5\"\n",
    "valid_y = \"data/camelyonpatch_level_2_split_valid_y.h5\"\n",
    "test_x  = \"data/camelyonpatch_level_2_split_test_x.h5\"\n",
    "test_y  = \"data/camelyonpatch_level_2_split_test_y.h5\"\n",
    "\n",
    "# Instantiate Datasets \n",
    "train_dataset = data_manipulation.PatchCamelyonH5Dataset(train_x, train_y, transform=transform)\n",
    "valid_dataset = data_manipulation.PatchCamelyonH5Dataset(valid_x, valid_y)\n",
    "test_dataset  = data_manipulation.PatchCamelyonH5Dataset(test_x, test_y)\n",
    "\n",
    "# Instatiate Loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Training a model from scratch would require a significant amount of time — over 10 hours on local hardware.\n",
    "To make the process more efficient and focused, we chose a different strategy: using a ResNet-18 backbone and training only its final classification head.\n",
    "\n",
    "This choice offers several advantages:\n",
    "- ResNet-18 is a lightweight yet powerful convolutional network, known for its solid performance even on relatively small datasets.\n",
    "- It has a simple and clean architecture, making it more interpretable than deeper models like ResNet-50 or ResNet-101.\n",
    "- Its relatively low number of parameters also reduces overfitting risk, especially important when working with limited data.\n",
    "\n",
    "To further mitigate overfitting, we restricted training to only 10% of the dataset, while ensuring stratified sampling. This way, the model is still exposed to examples from all classes, despite the small training set.\n",
    "\n",
    "In addition, we applied data augmentation:\n",
    "- Random rotations to simulate different orientations of tissue patches.\n",
    "- Color jitter to make the model more robust to variations in color and brightness that naturally occur in histology images.\n",
    "\n",
    "### Why ResNet-18?\n",
    "\n",
    "ResNet-18 strikes a balance between complexity and interpretability.\n",
    "It is deep enough to capture relevant spatial features but shallow enough to allow methods like Grad-CAM and Integrated Gradients to produce meaningful and localized explanations.\n",
    "Deeper models, while potentially more accurate, often create more diffused and harder-to-interpret explanation maps.\n",
    "\n",
    "Overall, ResNet-18 provides a solid foundation for balancing performance, training time, and explainability, which is crucial given the goals of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 0 to 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 4096/4096 [37:54<00:00,  1.80it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 0.3644\n",
      "Validation Accuracy: 0.8072\n",
      "Best model saved at epoch 1 with val_acc 0.8072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 4096/4096 [36:48<00:00,  1.85it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] Loss: 0.2491\n",
      "Validation Accuracy: 0.8711\n",
      "Best model saved at epoch 2 with val_acc 0.8711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 4096/4096 [32:43<00:00,  2.09it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] Loss: 0.2069\n",
      "Validation Accuracy: 0.8219\n",
      "EarlyStopping counter: 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 4096/4096 [28:45<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] Loss: 0.1831\n",
      "Validation Accuracy: 0.8719\n",
      "Best model saved at epoch 4 with val_acc 0.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 4096/4096 [24:37<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] Loss: 0.1659\n",
      "Validation Accuracy: 0.8637\n",
      "EarlyStopping counter: 1 of 3\n",
      "Training completed. Final model saved.\n"
     ]
    }
   ],
   "source": [
    "# Set to True only if you want to train again\n",
    "TRAIN = True\n",
    "\n",
    "device, checkpoint_path = model.setup_device_and_paths()\n",
    "resnet18, optimizer, criterion = model.initialize_model(device)\n",
    "resnet18, optimizer, start_epoch, best_val_acc = model.load_checkpoint(resnet18, optimizer, checkpoint_path, device)\n",
    "\n",
    "if TRAIN:\n",
    "    resnet18 = model.train_model(resnet18, optimizer, criterion, device,\n",
    "                        checkpoint_path, train_loader, val_loader,\n",
    "                        start_epoch=start_epoch, num_epochs=5, best_val_acc=best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicability\n",
    "\n",
    "To interpret the model’s predictions and understand where it focuses during decision-making, we selected three different explanation techniques:\n",
    "- Integrated Gradients\n",
    "- Grad-CAM\n",
    "- LIME\n",
    "\n",
    "Each method provides a different perspective on the model’s behavior, and together they give a broader view of the model’s reasoning process.\n",
    "\n",
    "### Integrated Gradients\n",
    "\n",
    "Integrated Gradients attribute the prediction by accumulating the gradients of the model’s output with respect to the input, along a straight path from a baseline (usually a black image) to the actual input.\n",
    "In simpler terms, it tells us which pixels contributed the most to the model’s decision.\n",
    "\n",
    "Pros:\n",
    "- Provides fine-grained, pixel-level attributions.\n",
    "- Theoretically well-founded, satisfying important axioms like sensitivity and implementation invariance.\n",
    "- Does not require any modification to the model architecture.\n",
    "\n",
    "Cons:\n",
    "- Produces dense attribution maps, which can be harder to visually interpret without further processing.\n",
    "- Requires defining a suitable baseline (black image, blurred image, etc.), and results can be sensitive to this choice.\n",
    "\n",
    "### Grad-CAM\n",
    "\n",
    "Grad-CAM (Gradient-weighted Class Activation Mapping) uses the gradients flowing into the last convolutional layers to produce a heatmap that highlights the important regions of the image for a given decision.\n",
    "Instead of focusing on individual pixels, it emphasizes higher-level spatial regions.\n",
    "\n",
    "Pros:\n",
    "- Produces smooth and localized explanations over meaningful areas.\n",
    "- Intuitive and easy to visualize, especially for CNNs.\n",
    "- Computationally efficient since it uses activations already computed during forward pass.\n",
    "\n",
    "Cons:\n",
    "- Can be less precise at the pixel level — focuses more on regions rather than fine structures.\n",
    "- Sensitive to the choice of the convolutional layer used for the explanation.\n",
    "\n",
    "### LIME\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) explains a single prediction by learning a simple, interpretable model (like a linear model) that approximates the original model’s behavior in the neighborhood of that prediction.\n",
    "It does so by perturbing the input and observing how the prediction changes.\n",
    "\n",
    "Pros:\n",
    "- Model-agnostic: can be applied to any classifier, not only neural networks.\n",
    "- Focuses on superpixels rather than individual pixels, making explanations visually intuitive.\n",
    "- Good for highlighting compact, highly-informative regions.\n",
    "\n",
    "Cons:\n",
    "- Computationally expensive, as it requires many forward passes with perturbed inputs.\n",
    "- Results can vary depending on the choice of perturbations and parameters like the number of samples.\n",
    "- Sometimes explanations can be unstable if the local approximation is not faithful enough.\n",
    "\n",
    "Summary\n",
    "\n",
    "Each method offers a complementary perspective:\n",
    "- Integrated Gradients focuses on detailed pixel contributions.\n",
    "- Grad-CAM highlights important spatial regions.\n",
    "- LIME captures local, interpretable areas based on perturbation analysis.\n",
    "\n",
    "Using all three methods allows for a more complete and reliable understanding of the model’s behavior, especially in critical domains such as medical imaging, where interpretability is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device, checkpoint_path = model.setup_device_and_paths()\n",
    "resnet18, optimizer, _ = model.initialize_model(device)\n",
    "resnet18, optimizer, start_epoch, best_val_acc = model.load_checkpoint(resnet18, optimizer, checkpoint_path, device)\n",
    "\n",
    "transform = explicability_pipeline.build_transforms()\n",
    "ig, cam, explainer = explicability_pipeline.build_explainers(resnet18)\n",
    "\n",
    "explicability_pipeline.run_explanations(resnet18, val_loader, ig, cam, explainer, transform, device, max_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Results\n",
    "\n",
    "<!-- Fig 1 -->\n",
    "<div align=\"center\">\n",
    "<img src=\"comparative_outputs/comparison_0.png\" alt=\"Grad-CAM / IG / LIME – image 0\">\n",
    "<span style=\"font-size:0.85em\"><b>Figure 1 — Panel 0</b></span>\n",
    "</div>\n",
    "\n",
    "<!-- Fig 2 -->\n",
    "<div align=\"center\">\n",
    "<img src=\"comparative_outputs/comparison_1.png\" alt=\"Grad-CAM / IG / LIME – image 1\">\n",
    "<span style=\"font-size:0.85em\"><b>Figure 2 — Panel 1</b></span>\n",
    "</div>\n",
    "\n",
    "<!-- Fig 3  (note: file-name updated) -->\n",
    "<div align=\"center\">\n",
    "<img src=\"comparative_outputs/comparison_6.png\" alt=\"Grad-CAM / IG / LIME – image 2\">\n",
    "<span style=\"font-size:0.85em\"><b>Figure 3 — Panel 2</b></span>\n",
    "</div>\n",
    "\n",
    "<!-- Fig 4 -->\n",
    "<div align=\"center\">\n",
    "<img src=\"comparative_outputs/comparison_4.png\" alt=\"Grad-CAM / IG / LIME – image 3\">\n",
    "<span style=\"font-size:0.85em\"><b>Figure 4 — Panel 3</b></span>\n",
    "</div>\n",
    "\n",
    "### Quick visual read-out\n",
    "- Fig 1 – All three methods converge on the same triangular region at the bottom-centre ⇒ strong, trustworthy evidence.\n",
    "- Fig 2 – Grad-CAM lights up the upper-left border while LIME outlines the lower-left edge; IG is almost uniform.\n",
    "Likely means the model is partially focusing on patch borders rather than tissue.\n",
    "- Fig 3 – Grad-CAM shows a coarse “cross” pattern (typical up-sampling artefact); IG is noisy; LIME marks two faint areas at the far edges.\n",
    "- Fig 4 – Consistent highlight: V-shaped region is marked by all three methods, reinforcing its anatomical relevance.\n",
    "\n",
    "### Shared trends vs. differences\n",
    "- Scale - Grad-CAM ≫ LIME (super-pixels) ≫ IG (pixels).\n",
    "- Noise - IG is inherently speckled; Grad-CAM/LIME are smoother.\n",
    "- Artefacts - Borders (Fig 2) and checkerboard blocks (Fig 3) are typical failure modes to watch for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from lime import lime_image\n",
    "\n",
    "# Grad-CAM wrapper\n",
    "target_layers = [resnet18.layer4[-1]]\n",
    "cam = GradCAM(model=resnet18, target_layers=target_layers, use_cuda=(device.type == \"cuda\"))\n",
    "\n",
    "def gc_expl(x, cls):\n",
    "    return cam(x, targets=[ClassifierOutputTarget(cls)])[0] # H×W\n",
    "\n",
    "# Integrated Gradients wrapper\n",
    "ig = IntegratedGradients(resnet18)\n",
    "\n",
    "def ig_expl(x, cls):\n",
    "    base = torch.zeros_like(x).to(device)\n",
    "    attr = ig.attribute(x, base, target=cls, n_steps=50)\n",
    "    attr = attr.squeeze().abs().mean(0).cpu().numpy() # H×W\n",
    "    return attr\n",
    "\n",
    "\n",
    "# LIME wrapper\n",
    "lime_exp = lime_image.LimeImageExplainer()\n",
    "\n",
    "def lime_expl(x, cls):\n",
    "    # tensor → uint8 RGB\n",
    "    d = x.squeeze().cpu()\n",
    "    d *= torch.tensor([0.229,0.224,0.225]).view(3,1,1)\n",
    "    d += torch.tensor([0.485,0.456,0.406]).view(3,1,1)\n",
    "    img = (d.clamp(0,1).permute(1,2,0).numpy()*255).astype(np.uint8)\n",
    "\n",
    "    exp = lime_exp.explain_instance(img,\n",
    "              lambda ims: explicability_pipeline.predict_fn(ims, resnet18, transform, device),\n",
    "              top_labels=2, hide_color=0, num_samples=1000)\n",
    "    mask = exp.get_image_and_mask(cls, positive_only=True,\n",
    "                                  num_features=5, hide_rest=False)[1]\n",
    "    return mask.astype(np.float32) # H×W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics_expl import evaluate_method\n",
    "\n",
    "for name, fn in [(\"Grad-CAM\", gc_expl),\n",
    "                 (\"Integrated Gradients\", ig_expl),\n",
    "                 (\"LIME\", lime_expl)]:\n",
    "\n",
    "    del_auc, ins_auc = evaluate_method(\n",
    "        model=resnet18,\n",
    "        loader=val_loader,\n",
    "        explainer_fn=fn,\n",
    "        name=name,\n",
    "        device=device,\n",
    "        max_imgs=30,          # evaluate on 30 validation images\n",
    "        steps=100             # 100 points on the curve\n",
    "    )\n",
    "\n",
    "    print(f\"{name:<20}  Deletion AUC: {del_auc:.4f}  |  Insertion AUC: {ins_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Grad-CAM:   0%|          | 0/512 [01:23<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad-CAM  Deletion AUC:  0.4580\n",
      "Grad-CAM  Insertion AUC: 0.5393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# 1.  Instantiate Grad-CAM\n",
    "target_layers = [resnet18.layer4[-1]]\n",
    "cam = GradCAM(model=resnet18, target_layers=target_layers)\n",
    "\n",
    "def gradcam_explainer(input_tensor: torch.Tensor, label: int) -> np.ndarray:\n",
    "    # Grad-CAM returns (1, H, W); we take [0]\n",
    "    return cam(input_tensor, targets=[ClassifierOutputTarget(label)])[0]\n",
    "\n",
    "# 2.  Run evaluation\n",
    "del_auc, ins_auc = explicability_pipeline.evaluate_method(\n",
    "    model=resnet18,\n",
    "    val_loader=val_loader,\n",
    "    explainer_fn=gradcam_explainer,\n",
    "    device=device,\n",
    "    method_name=\"Grad-CAM\",\n",
    "    steps=100,\n",
    "    max_images=30\n",
    ")\n",
    "\n",
    "print(f\"Grad-CAM  Deletion AUC:  {del_auc:.4f}\")\n",
    "print(f\"Grad-CAM  Insertion AUC: {ins_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [00:00<00:00, 2056.70it/s]?it/s]\n",
      "Evaluating Lime:   0%|          | 0/512 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "predict_fn() missing 3 required positional arguments: 'model', 'transform', and 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m     mask \u001b[38;5;241m=\u001b[39m explanation\u001b[38;5;241m.\u001b[39mget_image_and_mask(label, positive_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m                                           num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, hide_rest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)              \u001b[38;5;66;03m# H × W  (0/1 mask)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m del_auc, ins_auc \u001b[38;5;241m=\u001b[39m \u001b[43mexplicability_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet18\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlime_explainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLime  Deletion AUC:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdel_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLime  Insertion AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mins_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/CentraleSupelec/Mention/Rexia/Final Project/explicability_pipeline.py:200\u001b[0m, in \u001b[0;36mevaluate_method\u001b[0;34m(model, val_loader, explainer_fn, device, method_name, steps, max_images)\u001b[0m\n\u001b[1;32m    197\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# 1 × C × H × W\u001b[39;00m\n\u001b[1;32m    198\u001b[0m lbl_int \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(lbl)\n\u001b[0;32m--> 200\u001b[0m saliency \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbl_int\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# H × W\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# normalise saliency\u001b[39;00m\n\u001b[1;32m    203\u001b[0m saliency \u001b[38;5;241m=\u001b[39m (saliency \u001b[38;5;241m-\u001b[39m saliency\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (saliency\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m saliency\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mlime_explainer\u001b[0;34m(input_tensor, label)\u001b[0m\n\u001b[1;32m     10\u001b[0m denorm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m denorm \u001b[38;5;241m=\u001b[39m (denorm\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m---> 13\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mlime_exp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdenorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplicability_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhide_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m mask \u001b[38;5;241m=\u001b[39m explanation\u001b[38;5;241m.\u001b[39mget_image_and_mask(label, positive_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m                                       num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, hide_rest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/Documents/CentraleSupelec/Mention/Rexia/Final Project/.venv/lib/python3.9/site-packages/lime/lime_image.py:198\u001b[0m, in \u001b[0;36mLimeImageExplainer.explain_instance\u001b[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[1;32m    194\u001b[0m     fudged_image[:] \u001b[38;5;241m=\u001b[39m hide_color\n\u001b[1;32m    196\u001b[0m top \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m--> 198\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfudged_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mclassifier_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m distances \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mpairwise_distances(\n\u001b[1;32m    203\u001b[0m     data,\n\u001b[1;32m    204\u001b[0m     data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    205\u001b[0m     metric\u001b[38;5;241m=\u001b[39mdistance_metric\n\u001b[1;32m    206\u001b[0m )\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    208\u001b[0m ret_exp \u001b[38;5;241m=\u001b[39m ImageExplanation(image, segments)\n",
      "File \u001b[0;32m~/Documents/CentraleSupelec/Mention/Rexia/Final Project/.venv/lib/python3.9/site-packages/lime/lime_image.py:261\u001b[0m, in \u001b[0;36mLimeImageExplainer.data_labels\u001b[0;34m(self, image, fudged_image, segments, classifier_fn, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    259\u001b[0m imgs\u001b[38;5;241m.\u001b[39mappend(temp)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(imgs) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[0;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     labels\u001b[38;5;241m.\u001b[39mextend(preds)\n\u001b[1;32m    263\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_fn() missing 3 required positional arguments: 'model', 'transform', and 'device'"
     ]
    }
   ],
   "source": [
    "# LIME wrapper (superpixel explanation)\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "lime_exp = lime_image.LimeImageExplainer()\n",
    "\n",
    "def lime_explainer(input_tensor, label):\n",
    "    # Convert tensor to uint8 RGB\n",
    "    denorm = input_tensor.squeeze().cpu().clone()\n",
    "    denorm *= torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    denorm += torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    denorm = (denorm.clamp(0,1).permute(1,2,0).numpy()*255).astype(np.uint8)\n",
    "\n",
    "    explanation = lime_exp.explain_instance(\n",
    "        denorm, explicability_pipeline.predict_fn, top_labels=2, hide_color=0, num_samples=1000)\n",
    "    mask = explanation.get_image_and_mask(label, positive_only=True,\n",
    "                                          num_features=5, hide_rest=False)[1]\n",
    "    return mask.astype(np.float32)              # H × W  (0/1 mask)\n",
    "\n",
    "del_auc, ins_auc = explicability_pipeline.evaluate_method(\n",
    "    model=resnet18,\n",
    "    val_loader=val_loader,\n",
    "    explainer_fn=lime_explainer,\n",
    "    device=device,\n",
    "    method_name=\"Lime\",\n",
    "    steps=100,\n",
    "    max_images=30\n",
    ")\n",
    "\n",
    "print(f\"Lime  Deletion AUC:  {del_auc:.4f}\")\n",
    "print(f\"Lime  Insertion AUC: {ins_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated Gradients wrapper\n",
    "from captum.attr import IntegratedGradients\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "def ig_explainer(input_tensor, label):\n",
    "    baseline = torch.zeros_like(input_tensor).to(device)\n",
    "    attributions = ig.attribute(input_tensor, baseline, target=label, n_steps=50)\n",
    "    attr = attributions.squeeze().abs().mean(dim=0).cpu().numpy()  # H × W\n",
    "    return attr\n",
    "\n",
    "del_auc, ins_auc = explicability_pipeline.evaluate_method(\n",
    "    model=resnet18,\n",
    "    val_loader=val_loader,\n",
    "    explainer_fn=gradcam_explainer,\n",
    "    device=device,\n",
    "    method_name=\"Grad-CAM\",\n",
    "    steps=100,\n",
    "    max_images=30\n",
    ")\n",
    "\n",
    "print(f\"Grad-CAM  Deletion AUC:  {del_auc:.4f}\")\n",
    "print(f\"Grad-CAM  Insertion AUC: {ins_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
