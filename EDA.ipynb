{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rexia Final Project\n",
    "See description at README.md\n",
    "\n",
    "### Authors\n",
    "- Gabriel Souza Lima\n",
    "- Augustin Cobena\n",
    "\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import data_manipulation\n",
    "import model\n",
    "import explicability_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "base = \"https://zenodo.org/record/2546921/files\"\n",
    "files = [\n",
    "    \"camelyonpatch_level_2_split_train_x.h5\",\n",
    "    \"camelyonpatch_level_2_split_train_y.h5\",\n",
    "    \"camelyonpatch_level_2_split_valid_x.h5\",\n",
    "    \"camelyonpatch_level_2_split_valid_y.h5\",\n",
    "    \"camelyonpatch_level_2_split_test_x.h5\",\n",
    "    \"camelyonpatch_level_2_split_test_y.h5\",\n",
    "]\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "for fname in files:\n",
    "    out_path = os.path.join(\"data\", fname)\n",
    "    if not os.path.exists(out_path):\n",
    "        print(f\"Downloading {fname}...\")\n",
    "        url = f\"{base}/{fname}.gz?download=1\"\n",
    "        os.system(f\"curl -L -o {out_path} '{url}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.2, 0.2, 0.2)], p=0.5)\n",
    "])\n",
    "\n",
    "# Downloaded files \n",
    "train_x = \"data/camelyonpatch_level_2_split_train_x.h5\"\n",
    "train_y = \"data/camelyonpatch_level_2_split_train_y.h5\"\n",
    "valid_x = \"data/camelyonpatch_level_2_split_valid_x.h5\"\n",
    "valid_y = \"data/camelyonpatch_level_2_split_valid_y.h5\"\n",
    "test_x  = \"data/camelyonpatch_level_2_split_test_x.h5\"\n",
    "test_y  = \"data/camelyonpatch_level_2_split_test_y.h5\"\n",
    "\n",
    "# Instantiate Datasets \n",
    "train_dataset = data_manipulation.PatchCamelyonH5Dataset(train_x, train_y, transform=transform)\n",
    "valid_dataset = data_manipulation.PatchCamelyonH5Dataset(valid_x, valid_y)\n",
    "test_dataset  = data_manipulation.PatchCamelyonH5Dataset(test_x, test_y)\n",
    "\n",
    "# Instatiate Loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Training a model from scratch would require a significant amount of time — over 10 hours on local hardware.\n",
    "To make the process more efficient and focused, we chose a different strategy: using a ResNet-18 backbone and training only its final classification head - which reduced training to 2.5 hours.\n",
    "\n",
    "This choice offers several advantages:\n",
    "- ResNet-18 is a lightweight yet powerful convolutional network, known for its solid performance even on relatively small datasets.\n",
    "- It has a simple and clean architecture, making it more interpretable than deeper models like ResNet-50 or ResNet-101.\n",
    "- Its relatively low number of parameters also reduces overfitting risk, especially important when working with limited data.\n",
    "\n",
    "To further mitigate overfitting, we restricted training to only 10% of the dataset, while ensuring stratified sampling. This way, the model is still exposed to examples from all classes, despite the small training set.\n",
    "\n",
    "In addition, we applied data augmentation:\n",
    "- Random rotations to simulate different orientations of tissue patches.\n",
    "- Color jitter to make the model more robust to variations in color and brightness that naturally occur in histology images.\n",
    "\n",
    "### Why ResNet-18?\n",
    "\n",
    "ResNet-18 strikes a balance between complexity and interpretability.\n",
    "It is deep enough to capture relevant spatial features but shallow enough to allow methods like Grad-CAM and Integrated Gradients to produce meaningful and localized explanations.\n",
    "Deeper models, while potentially more accurate, often create more diffused and harder-to-interpret explanation maps.\n",
    "\n",
    "Overall, ResNet-18 provides a solid foundation for balancing performance, training time, and explainability, which is crucial given the goals of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 0 to 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 4096/4096 [37:54<00:00,  1.80it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 0.3644\n",
      "Validation Accuracy: 0.8072\n",
      "Best model saved at epoch 1 with val_acc 0.8072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 4096/4096 [36:48<00:00,  1.85it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] Loss: 0.2491\n",
      "Validation Accuracy: 0.8711\n",
      "Best model saved at epoch 2 with val_acc 0.8711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 4096/4096 [32:43<00:00,  2.09it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] Loss: 0.2069\n",
      "Validation Accuracy: 0.8219\n",
      "EarlyStopping counter: 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 4096/4096 [28:45<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] Loss: 0.1831\n",
      "Validation Accuracy: 0.8719\n",
      "Best model saved at epoch 4 with val_acc 0.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 4096/4096 [24:37<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] Loss: 0.1659\n",
      "Validation Accuracy: 0.8637\n",
      "EarlyStopping counter: 1 of 3\n",
      "Training completed. Final model saved.\n"
     ]
    }
   ],
   "source": [
    "# Set to True only if you want to train again\n",
    "TRAIN = False\n",
    "\n",
    "device, checkpoint_path = model.setup_device_and_paths()\n",
    "resnet18, optimizer, criterion = model.initialize_model(device)\n",
    "resnet18, optimizer, start_epoch, best_val_acc = model.load_checkpoint(resnet18, optimizer, checkpoint_path, device)\n",
    "\n",
    "if TRAIN:\n",
    "    resnet18 = model.train_model(resnet18, optimizer, criterion, device,\n",
    "                        checkpoint_path, train_loader, val_loader,\n",
    "                        start_epoch=start_epoch, num_epochs=5, best_val_acc=best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicability\n",
    "\n",
    "To interpret the model’s predictions and understand where it focuses during decision-making, we selected three different explanation techniques:\n",
    "- Integrated Gradients\n",
    "- Grad-CAM\n",
    "- LIME\n",
    "\n",
    "Each method provides a different perspective on the model’s behavior, and together they give a broader view of the model’s reasoning process.\n",
    "\n",
    "### Integrated Gradients\n",
    "\n",
    "Integrated Gradients attribute the prediction by accumulating the gradients of the model’s output with respect to the input, along a straight path from a baseline (usually a black image) to the actual input.\n",
    "In simpler terms, it tells us which pixels contributed the most to the model’s decision.\n",
    "\n",
    "Pros:\n",
    "- Provides fine-grained, pixel-level attributions.\n",
    "- Theoretically well-founded, satisfying important axioms like sensitivity and implementation invariance.\n",
    "- Does not require any modification to the model architecture.\n",
    "\n",
    "Cons:\n",
    "- Produces dense attribution maps, which can be harder to visually interpret without further processing.\n",
    "- Requires defining a suitable baseline (black image, blurred image, etc.), and results can be sensitive to this choice.\n",
    "\n",
    "### Grad-CAM\n",
    "\n",
    "Grad-CAM (Gradient-weighted Class Activation Mapping) uses the gradients flowing into the last convolutional layers to produce a heatmap that highlights the important regions of the image for a given decision.\n",
    "Instead of focusing on individual pixels, it emphasizes higher-level spatial regions.\n",
    "\n",
    "Pros:\n",
    "- Produces smooth and localized explanations over meaningful areas.\n",
    "- Intuitive and easy to visualize, especially for CNNs.\n",
    "- Computationally efficient since it uses activations already computed during forward pass.\n",
    "\n",
    "Cons:\n",
    "- Can be less precise at the pixel level — focuses more on regions rather than fine structures.\n",
    "- Sensitive to the choice of the convolutional layer used for the explanation.\n",
    "\n",
    "### LIME\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) explains a single prediction by learning a simple, interpretable model (like a linear model) that approximates the original model’s behavior in the neighborhood of that prediction.\n",
    "It does so by perturbing the input and observing how the prediction changes.\n",
    "\n",
    "Pros:\n",
    "- Model-agnostic: can be applied to any classifier, not only neural networks.\n",
    "- Focuses on superpixels rather than individual pixels, making explanations visually intuitive.\n",
    "- Good for highlighting compact, highly-informative regions.\n",
    "\n",
    "Cons:\n",
    "- Computationally expensive, as it requires many forward passes with perturbed inputs.\n",
    "- Results can vary depending on the choice of perturbations and parameters like the number of samples.\n",
    "- Sometimes explanations can be unstable if the local approximation is not faithful enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found. Loading model...\n",
      "Model loaded. Resuming from epoch 4, best validation acc = 0.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 210.68it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 214.98it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 212.82it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 217.30it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 217.01it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 217.97it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 220.20it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 218.58it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 219.27it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 216.92it/s]\n"
     ]
    }
   ],
   "source": [
    "device, checkpoint_path = model.setup_device_and_paths()\n",
    "resnet18, optimizer, _ = model.initialize_model(device)\n",
    "resnet18, optimizer, start_epoch, best_val_acc = model.load_checkpoint(resnet18, optimizer, checkpoint_path, device)\n",
    "\n",
    "transform = explicability_pipeline.build_transforms()\n",
    "ig, cam, explainer = explicability_pipeline.build_explainers(resnet18)\n",
    "\n",
    "explicability_pipeline.run_explanations(resnet18, val_loader, ig, cam, explainer, transform, device, max_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Results\n",
    "<!-- Fig 1 -->\n",
    "<div align=\"center\">\n",
    "<img src=\"comparative_outputs/comparison_0_new.png\" alt=\"Grad-CAM / IG / LIME – image 0\">\n",
    "<span style=\"font-size:0.85em\"><b>Figure 1 — Panel 0</b></span>\n",
    "</div>\n",
    "\n",
    "<!-- Fig 2 -->\n",
    "<div align=\"center\">\n",
    "<img src=\"comparative_outputs/comparison_3_new.png\" alt=\"Grad-CAM / IG / LIME – image 1\">\n",
    "<span style=\"font-size:0.85em\"><b>Figure 2 — Panel 1</b></span>\n",
    "</div>\n",
    "\n",
    "<!-- Fig 3  (note: file-name updated) -->\n",
    "<div align=\"center\">\n",
    "<img src=\"comparative_outputs/comparison_6_new.png\" alt=\"Grad-CAM / IG / LIME – image 2\">\n",
    "<span style=\"font-size:0.85em\"><b>Figure 3 — Panel 2</b></span>\n",
    "</div>\n",
    "\n",
    "<!-- Fig 4 -->\n",
    "<div align=\"center\">\n",
    "<img src=\"comparative_outputs/comparison_9_new.png\" alt=\"Grad-CAM / IG / LIME – image 3\">\n",
    "<span style=\"font-size:0.85em\"><b>Figure 4 — Panel 3</b></span>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Fig. 1\n",
    "- Grad-CAM shows a strong, rounded hot-spot in the upper-left corner.\n",
    "- Integrated Gradients also concentrates energy in that same corner, with a tight cluster of bright pixels.\n",
    "- LIME draws a broad yellow contour that encloses almost the same anatomical region (although its outline is less precise).\n",
    "\n",
    "Take-away: all three methods agree on the upper-left patch being the main driver of the prediction; the overlap is remarkably tight, so this region is very likely relevant for the network.\n",
    "\n",
    "---\n",
    "\n",
    "### Fig. 2\n",
    "- Grad-CAM highlights a tall triangular ridge running from bottom-left to top-centre.\n",
    "- Integrated Gradients places its brightest pixels inside that ridge, but in a much smaller inner core.\n",
    "- LIME detects only a thin super-pixel contour on the lower-right edge, not touching the Grad-CAM ridge.\n",
    "\n",
    "Interpretation: IG partially corroborates Grad-CAM (same ridge, smaller focus), whereas LIME largely misses it—possibly because the SLIC segmentation breaks the ridge into many small pieces and only one of them meets LIME’s “top features” threshold.\n",
    "\n",
    "---\n",
    "\n",
    "### Fig. 3\n",
    "- Grad-CAM lights up an L-shaped corner (bottom-right) with a very smooth gradient; that pattern looks like an up-sampling artefact rather than tissue detail.\n",
    "- Integrated Gradients instead highlights the opposite corner (top-left) with a dense block of bright pixels.\n",
    "- LIME outlines a long vertical band on the far right side; again it sits on the Grad-CAM corner, not on the IG cluster.\n",
    "\n",
    "Interpretation: the three methods disagree. Given the blocky shape in Grad-CAM and the mismatch with IG, the highlighted regions may be artefactual (patch border, staining variation) rather than true histological signal. Switching Grad-CAM to a slightly earlier convolutional layer could clarify the map.\n",
    "\n",
    "----\n",
    "\n",
    "### Fig. 4\n",
    "- Grad-CAM shows a wide colour band sweeping diagonally across the patch.\n",
    "- Integrated Gradients scatters multiple bright hotspots along that same band—good positional agreement, albeit noisier.\n",
    "- LIME draws two disconnected contours sitting on the upper-right and lower-left ends of the band.\n",
    "\n",
    "Interpretation: all three methods locate the same diagonal structure, but they “slice” it differently: Grad-CAM gives the full band, IG pin-points small sub-regions, and LIME captures only the extreme ends (largest super-pixels).\n",
    "\n",
    "---\n",
    "\n",
    "### Cross-figure observations\n",
    "1.\tScale hierarchy holds everywhere\n",
    "Grad-CAM paints the broadest areas, IG the finest details, LIME sits in-between (super-pixels).\n",
    "2.\tAgreement is high in Fig. 1 and Fig. 4, partial in Fig. 2, poor in Fig. 3.\n",
    "Divergence usually indicates either segmentation artefacts (LIME) or coarse feature-map artefacts (Grad-CAM).\n",
    "3.\tWhen IG and Grad-CAM coincide (Fig. 1), confidence in that region being class-discriminative is strong.\n",
    "4.\tLIME is sensitive to super-pixel size—in Fig. 2 and Fig. 3 it misses narrow structures that IG detects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from lime import lime_image\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# Grad-CAM wrapper\n",
    "target_layers = [resnet18.layer4[-1]]\n",
    "cam = GradCAM(model=resnet18, target_layers=target_layers)\n",
    "\n",
    "def gc_expl(x, cls):\n",
    "    return cam(x, targets=[ClassifierOutputTarget(cls)])[0] # H×W\n",
    "\n",
    "ig = IntegratedGradients(resnet18)\n",
    "\n",
    "def ig_expl(x, cls):\n",
    "    \"\"\"Return a 2-D NumPy saliency map (H×W).\"\"\"\n",
    "    baseline = torch.zeros_like(x).to(x.device)\n",
    "    # attr: 1×C×H×W  (tensor on the same device as x)\n",
    "    attr = ig.attribute(x, target=cls, baselines=baseline, n_steps=50)\n",
    "    # collapse channel-dimension → H×W, detach, move to CPU, convert to NumPy\n",
    "    sal = attr.squeeze(0).abs().mean(0).detach().cpu().numpy()\n",
    "    return sal                   \n",
    "\n",
    "# LIME wrapper\n",
    "lime_exp = lime_image.LimeImageExplainer()\n",
    "\n",
    "def lime_expl(x, cls):\n",
    "    # tensor → uint8 RGB\n",
    "    d = x.squeeze().cpu()\n",
    "    d *= torch.tensor([0.229,0.224,0.225]).view(3,1,1)\n",
    "    d += torch.tensor([0.485,0.456,0.406]).view(3,1,1)\n",
    "    img = (d.clamp(0,1).permute(1,2,0).numpy()*255).astype(np.uint8)\n",
    "\n",
    "    exp = lime_exp.explain_instance(img,\n",
    "              lambda ims: explicability_pipeline.predict_fn(ims, resnet18, transform, device),\n",
    "              top_labels=2, hide_color=0, num_samples=1000)\n",
    "    mask = exp.get_image_and_mask(cls, positive_only=True,\n",
    "                                  num_features=5, hide_rest=False)[1]\n",
    "    return mask.astype(np.float32) # H×W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Grad-CAM:   0%|          | 0/512 [01:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad-CAM              Deletion AUC: 0.3885  |  Insertion AUC: 0.8014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Integrated Gradients:   0%|          | 0/512 [01:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated Gradients  Deletion AUC: 0.6218  |  Insertion AUC: 0.6359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 213.84it/s]t/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 212.77it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 217.09it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 216.25it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 216.39it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 216.26it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 205.19it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 219.00it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 215.31it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 216.71it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 218.12it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 215.77it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 217.50it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 215.42it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 212.34it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 216.57it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 216.42it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 217.21it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 218.93it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 215.74it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 214.94it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 213.91it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 215.20it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 214.72it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 218.46it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 217.60it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 215.98it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 215.49it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 217.00it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 209.72it/s]\n",
      "Evaluating LIME:   0%|          | 0/512 [03:25<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME                  Deletion AUC: 0.3428  |  Insertion AUC: 0.7215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for name, fn in [(\"Grad-CAM\", gc_expl),\n",
    "                 (\"Integrated Gradients\", ig_expl),\n",
    "                 (\"LIME\", lime_expl)]:\n",
    "\n",
    "    del_auc, ins_auc = explicability_pipeline.evaluate_method(\n",
    "        model=resnet18,\n",
    "        loader=val_loader,\n",
    "        explainer_fn=fn,\n",
    "        name=name,\n",
    "        device=device,\n",
    "        max_imgs=30, # evaluate on 30 validation images\n",
    "        steps=100 # 100 points on the curve\n",
    "    )\n",
    "\n",
    "    print(f\"{name:<20}  Deletion AUC: {del_auc:.4f}  |  Insertion AUC: {ins_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative sanity-check: Deletion & Insertion AUCs\n",
    "\n",
    "| Method                 | Deletion AUC ↓ (lower = better) | Insertion AUC ↑ (higher = better) |\n",
    "|-------------------------|---------------------------------|-----------------------------------|\n",
    "| **Grad-CAM**            | 0.39                            | 0.80                             |\n",
    "| **Integrated Gradients**| 0.62                            | 0.64                             |\n",
    "| **LIME**                | 0.34                            | 0.72                             |\n",
    "\n",
    "### Interpretation rule of thumb\n",
    "- Deletion AUC: how fast confidence drops when we remove the most-important pixels. Lower = saliency map really captured what the model needs.\n",
    "- Insertion AUC: how fast confidence rises when we re-insert important pixels. Higher = map contains true evidence.\n",
    "\n",
    "---\n",
    "\n",
    "### What the numbers tell us\n",
    "1.\tGrad-CAM dominates insertion (0.80)\n",
    "- The model’s confidence recovers quickly as soon as the Grad-CAM mask is pasted back → its blobs indeed cover decisive regions.\n",
    "- This echoes Fig. 1 and Fig. 4, where Grad-CAM highlighted the exact class-driving tissue.\n",
    "2.\tLIME wins the deletion race (0.34)\n",
    "- Once LIME’s super-pixels are blacked out, confidence collapses fastest.\n",
    "- Even though LIME sometimes missed fine structures (Fig. 2), the segments it did choose are clearly essential.\n",
    "3.\tIntegrated Gradients is the most conservative\n",
    "- Middle-of-the-road scores (0.62 / 0.64) suggest its pixel-wise heat map spreads importance more thinly; removing or adding small fractions doesn’t move the needle as sharply.\n",
    "- That matches our qualitative note that IG looks “noisier” and often extends into background (Fig. 3).\n",
    "\n",
    "---\n",
    "\n",
    "### Linking back to the four visual panels\n",
    "\n",
    "| Panel                        | Qualitative verdict                     | AUC behaviour explained                                                |\n",
    "|-------------------------------|-----------------------------------------|------------------------------------------------------------------------|\n",
    "| **Fig. 1 (tight agreement)**  | All three masks centred on same hot-spot | High Grad-CAM & LIME insertion; deletion hurts quickly for any method  |\n",
    "| **Fig. 2 (LIME under-covers ridge)** | Grad-CAM + IG agree, LIME partial      | Grad-CAM insertion high; LIME still good at deletion because its few segments are truly critical |\n",
    "| **Fig. 3 (Grad-CAM artefact)** | Methods disagree; Grad-CAM blocky       | Grad-CAM still useful (AUC 0.80) but deletion score worse (0.39) than LIME’s 0.34, showing artefact cost |\n",
    "| **Fig. 4 (broad diagonal band)** | Methods align again                   | Consistent with strong Grad-CAM metrics and decent IG/LIME scores       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
